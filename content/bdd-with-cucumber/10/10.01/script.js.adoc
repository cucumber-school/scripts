=== Tidy up the bug report Gherkin

Let's start by running our failing scenario.shot::[1] We have it marked with a `@todo` tag on it,shot::[2] but those scenarios are filtered out in our default `npm test` command, so let's make a new one.

[source,js]
----
include::../code/js/00-initial-commit/package.json[lines=6..9]
----

Now we can use that to run everthing including our `@todo` scenario: shot::[3]

[source, sh]
----
$ npm run test:all

> shouty@1.0.0 test:all
> mocha test && cucumber-js



  Network
    ✓ broadcasts a message to a listener within range
    ✓ does not broadcast a message to a listener out of range
    ✓ does not broadcast a message to a listener out of range negative distance
    ✓ does not broadcast a message over 180 characters even if listener is in range
    ✓ can change the range
    credits
      ✓ deducts 2 credits for a shout over 180 characters

  Person
    ✓ subscribes to the network
    ✓ has a location
    ✓ broadcasts shouts to the network
    ✓ remembers messages heard
    ✓ can be moved to a different location


  11 passing (9ms)

........................................F

Failures:

1) Scenario: BUG #2789 # features/premium_shouts.feature:28
   ✔ Given the range is 100 # features/step_definitions/steps.js:4
   ✔ And Sean is located at 0 # features/step_definitions/steps.js:8
   ✔ And Lucy is located at 100 # features/step_definitions/steps.js:8
   ✔ Given Sean has bought 30 credits # features/step_definitions/steps.js:12
   ✔ When Sean shouts "buy, buy buy!" # features/step_definitions/shout_steps.js:26
   ✖ Then Sean should have 25 credits # features/step_definitions/steps.js:42
       AssertionError: 
       Expected: is <25>
            but: was <15>
           + expected - actual

           -15
           +25

           at ShoutyWorld.<anonymous> (/Users/matt/git/github.com/cucumber-school/scripts/code.bdd-with-cucumber.10.js/features/step_definitions/steps.js:43:3)

7 scenarios (1 failed, 6 passed)
41 steps (1 failed, 40 passed)
0m00.027s (executing steps: 0m00.002s)
----

OK. We'll work outside-in and start by tidying up the Gherkin specification.

Right now, the scenario is still in the raw form it was in when the bug was first reported, with a name that references an ID in our bug tracking system.shot::[4] This doesn't make for very good documentation about the intended behaviour.

[source,js]
----
include::../code/js/00-initial-commit/features/premium_shouts.feature[lines=27..31]
----

It also isn't sitting under the right rule.

Let's start by moving it here, under the rule about charging for mentioning the word "buy" shot::[5, move scenario under first rule, run tests]

[source,js]
----
include::../code/js/01-move-scenario-under-the-right-rule/features/premium_shouts.feature[lines=11..31]
----

Now let's find a better name for the scenario.

Using our Friends Episode naming convention that we introduced in Chapter 7, we could call it something like "(the one where Sean) mentions “buy” several times in one shout"? shot::[6, change the scenario title]shot::[7, run tests]

[source,js]
----
include::../code/js/02-rename-scenario/features/premium_shouts.feature[lines=20]
----

You might be worried about losing this bug ID. We could keep it in a comment, a tag, or in the description of the Scenario.

Just like under a `Feature` keyword, you can write any arbitrary text you want beneath a `Scenario` keyword, before the first `Given`, `When` or `Then` keyword. 

So let's do that. shot::[8, write the bug number in the scenario description, run tests]

[source,js]
----
include::../code/js/02-rename-scenario/features/premium_shouts.feature[lines=19..22]
----

We think the values in the example could be changed to make it a little more expressive. If we start Sean off with 100 credits,shot::[9] and end him with 95,shot::[10] it more clearly illustrates the rule that only five credits should be deducted. shot::[11, run tests]

[source,js]
----
include::../code/js/03-changed-values-in-scenario/features/premium_shouts.feature[lines=19..24]
----

OK great, so now we have a failing test we're more happy with.

The trouble is, although this test tells us what's wrong with the behaviour of the system - the wrong number of credits are being deducted - it doesn't give us any clues as to *why*.

This is why we need a balance of unit tests _and_ acceptance tests: when they fail, acceptance tests tell you what is wrong with the system, but unit tests tell you where you need to go to fix it.

